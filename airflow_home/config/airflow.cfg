[core]
# The home folder for airflow, default is ~/airflow
airflow_home = C:\xampp\htdocs\edusight\airflow_home

# The folder where your airflow pipelines live
dags_folder = C:\xampp\htdocs\edusight\airflow_home\dags

# The folder where airflow should store its log files
base_log_folder = C:\xampp\htdocs\edusight\airflow_home\logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
remote_logging = False

# Logging level
logging_level = INFO

# Fab logging level
fab_logging_level = WARN

# The executor class that airflow should use.
executor = LocalExecutor

# The SqlAlchemy connection string to the metadata database.
sql_alchemy_conn = sqlite:///C:\xampp\htdocs\edusight\airflow_home\airflow.db

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# When not using pools, tasks are run in the "default pool"
non_pooled_task_slot_count = 128

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the examples that ship with Airflow
load_examples = False

# Whether to load the default connections that ship with Airflow
load_default_connections = True

# Path to custom plugins
plugins_folder = C:\xampp\htdocs\edusight\airflow_home\plugins

# Secret key to save connection passwords in the db
fernet_key = 

# Whether to disable pickling dags
donot_pickle = False

# How long before timing out a python file import while filling the DagBag
dagbag_import_timeout = 30

[scheduler]
# Task instances listen for external kill signal (when you `airflow tasks run`),
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks
scheduler_heartbeat_sec = 5

# after how much time should the scheduler terminate in seconds (-1 indicates to run continuously)
run_duration = -1

# after how much time a new DAGs should be picked up from the filesystem
min_file_process_interval = 0

# How often should stats be printed to the logs
print_stats_interval = 30

# If the last scheduler heartbeat happened more than scheduler_health_check_threshold ago (in seconds),
# scheduler is considered unhealthy.
scheduler_health_check_threshold = 30

# How often (in seconds) to check for orphaned tasks and SchedulerJobs
orphaned_tasks_check_interval = 300.0

# How often (in seconds) to check for DAGs that should be deactivated
child_process_log_directory = C:\xampp\htdocs\edusight\airflow_home\logs\scheduler

# Local task jobs periodically heartbeat to the DB
scheduler_zombie_task_threshold = 300

# Turn off scheduler catchup by setting this to False
catchup_by_default = True

# This changes the batch size of queries in the scheduling main loop
max_tis_per_query = 512

[webserver]
# The base url of your website as airflow cannot guess what domain or
base_url = http://localhost:8080

# Default timezone to display all dates in the WEBUI, can be UTC, system, or
default_ui_timezone = UTC

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server
web_server_ssl_cert = 
web_server_ssl_key = 

# Number of seconds the webserver waits before killing gunicorn master that doesn't respond
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to refresh at a time
worker_refresh_batch_size = 1

# Number of workers to refresh at a time
worker_refresh_interval = 30

# Secret key used to run your flask app
secret_key = temporary_key

# Number of workers to run the Gunicorn web server
workers = 4

# The worker class gunicorn should use
worker_class = sync

# Log files for the gunicorn webserver
access_logfile = -
error_logfile = -

# Expose the configuration file in the web server
expose_config = False

# Default DAG view
dag_default_view = tree

# Default DAG orientation
dag_orientation = LR

# Puts the webserver in demonstration mode
demo_mode = False

# The amount of time (in secs) webserver will wait for initial handshake
web_server_worker_timeout = 120

# Set to true to turn on authentication
authenticate = False

# Filter the list of dags by owner name (requires authentication to be enabled)
filter_by_owner = False

# Filtering mode
owner_mode = user

[email]
email_backend = airflow.utils.email.send_email_smtp

[smtp]
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 587
smtp_mail_from = airflow@edusight.com

[celery]
# This section only applies if you are using the CeleryExecutor in [core] section above
celery_app_name = airflow.executors.celery_executor
worker_concurrency = 16
worker_log_server_port = 8793
broker_url = redis://localhost:6379/0
result_backend = redis://localhost:6379/0
flower_host = 0.0.0.0
flower_port = 5555
default_queue = default

[operators]
# The default owner assigned to each new operator, unless provided explicitly or passed via `default_args`
default_owner = Airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0
